[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "joblib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "joblib",
        "description": "joblib",
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "recall_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "roc_auc_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "RandomForestClassifier",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "stop",
        "importPath": "tracemalloc",
        "description": "tracemalloc",
        "isExtraImport": true,
        "detail": "tracemalloc",
        "documentation": {}
    },
    {
        "label": "LabelEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "PorterStemmer",
        "importPath": "nltk.stem.porter",
        "description": "nltk.stem.porter",
        "isExtraImport": true,
        "detail": "nltk.stem.porter",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "def load_data(data_url: str) -> pd.DataFrame:\n    \"\"\"load data from csv file\"\"\"\n    try:\n        df=pd.read_csv(data_url)\n        logging.debug(f\"Data loaded from {data_url}\")\n        return df\n    except pd.errors.ParserError as e:\n        logging.error(f\"Error loading data from {data_url}: {e}\")\n        raise\n    except Exception as e:",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "preprocess_data",
        "kind": 2,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "def preprocess_data(df:pd.DataFrame) -> pd.DataFrame:\n    \"\"\"preprocess data\"\"\"\n    try:\n        df.drop(columns = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)\n        df.rename(columns = {'v1':'target','v2': 'text'}, inplace=True)\n        logging.debug(\"Data preprocessing completed\")\n        return df\n    except KeyError as e:\n        logging.error(f\"Missing Columns in dataframe: {e}\")\n        raise",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "save_data",
        "kind": 2,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "def save_data(train_data:pd.DataFrame, test_data:pd.DataFrame, data_path: str) -> None:\n    \"\"\"save data to train and split\"\"\"\n    try:\n        raw_data_path = os.path.join(data_path,'raw')\n        os.makedirs(raw_data_path, exist_ok=True)\n        train_data.to_csv(os.path.join(raw_data_path, 'train.csv'), index=False)\n        test_data.to_csv(os.path.join(raw_data_path, 'test.csv'), index=False)\n        logging.debug(\"Data saved to csv file %s\", raw_data_path)\n    except Exception as e:\n        logging.error(f\"Error saving data to csv file: {e}\")",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "def main():\n    try:\n        test_size=0.2\n        data_path='https://raw.githubusercontent.com/vikashishere/Datasets/main/spam.csv'\n        df = load_data(data_path)\n        final_df = preprocess_data(df)\n        train_data, test_data = train_test_split(final_df, test_size=test_size, random_state=2)\n        save_data(train_data, test_data, data_path='./data')\n    except Exception as e:\n        logging.error(f\"Error in main function: {e}\")",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "log_dir = 'logs'\nos.makedirs(log_dir,exist_ok=True)\nlogger=logging.getLogger('data_ingestion')\nlogger.setLevel('DEBUG')\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"data_ingestion.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "console_handler",
        "kind": 5,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "console_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"data_ingestion.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "file_handler",
        "kind": 5,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "file_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Data Ingestion Started')\nlogger.info('Data Ingestion Completed')\nlogger.info('Data Ingestion Completed')",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "src.data_ingestion",
        "description": "src.data_ingestion",
        "peekOfCode": "formatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Data Ingestion Started')\nlogger.info('Data Ingestion Completed')\nlogger.info('Data Ingestion Completed')\ndef load_data(data_url: str) -> pd.DataFrame:\n    \"\"\"load data from csv file\"\"\"",
        "detail": "src.data_ingestion",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "def load_data(file_path: str) -> pd.DataFrame:\n    \"\"\"load data from csv file\"\"\"\n    try:\n        df=pd.read_csv(file_path)\n        logging.debug(f\"Data loaded from {file_path}\")\n        df.fillna('', inplace=True)\n        return df\n    except pd.errors.ParserError as e:\n        logging.error(f\"failed to parse the file {file_path}: {e}\")\n        raise",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "apply_tfidf",
        "kind": 2,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "def apply_tfidf(train_data:pd.DataFrame, test_data: pd.DataFrame, max_features:int) -> tuple:\n    \"\"\"apply tfidf vectorizer on train and test data\"\"\"\n    try:\n        vectorizer = TfidfVectorizer(max_features=max_features)\n        X_train = train_data['text'].values\n        y_train = train_data['target'].values\n        X_test = test_data['text'].values\n        y_test = test_data['target'].values\n        X_train_bow = vectorizer.fit_transform(X_train)\n        X_test_bow = vectorizer.transform(X_test)",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "save_data",
        "kind": 2,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "def save_data(df:pd.DataFrame, file_path: str):\n    \"\"\"save data to csv file\"\"\"\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        df.to_csv(file_path, index=False)\n        logging.debug(f\"Data saved to {file_path}\")\n    except Exception as e:\n        logging.error(f\"Error saving data to {file_path}: {e}\")\n        raise\ndef main():",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "def main():\n    try:\n        max_features = 50\n        train_data = load_data('./data/processed/interim/train_processed.csv')\n        test_data = load_data('./data/processed/interim/test_processed.csv')\n        train_df, test_df = apply_tfidf(train_data, test_data, max_features)\n        save_data(train_df, os.path.join(\"./data\",\"processed\",\"train_tfidf.csv\"))\n        save_data(test_df, os.path.join(\"./data\",\"processed\",\"test_tfidf.csv\"))\n        logger.info('Feature Engineering Completed')\n    except Exception as e:",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "log_dir = 'logs'\nos.makedirs(log_dir,exist_ok=True)\nlogger=logging.getLogger('feature_engineering')\nlogger.setLevel('DEBUG')\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"feature_engineering.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "console_handler",
        "kind": 5,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "console_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"feature_engineering.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "file_handler",
        "kind": 5,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "file_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Feature Engineering Started')\ndef load_data(file_path: str) -> pd.DataFrame:\n    \"\"\"load data from csv file\"\"\"",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "src.feature_engineering",
        "description": "src.feature_engineering",
        "peekOfCode": "formatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Feature Engineering Started')\ndef load_data(file_path: str) -> pd.DataFrame:\n    \"\"\"load data from csv file\"\"\"\n    try:\n        df=pd.read_csv(file_path)",
        "detail": "src.feature_engineering",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "def load_model(model_path: str):\n    try:\n        with open(model_path, \"rb\") as model_file:\n            model = joblib.load(model_file)\n            return model\n    except Exception as e:\n        raise e\n    except FileNotFoundError as e:\n        raise e\ndef load_data(file_path: str) -> pd.DataFrame:",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "def load_data(file_path: str) -> pd.DataFrame:\n    try:\n        df=pd.read_csv(file_path)\n        logging.info(\"data loaded from %s file path of size %s\",file_path,df.shape)\n        return df\n    except pd.errors.ParserError as e:\n        logging.error(\"faied to parse the file%s\",e)\n        raise\n    except Exception as e:\n        logging.error(f\"Error loading data from {file_path}: {e}\")",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "def evaluate_model(clf, X_test:pd.DataFrame, y_test:pd.DataFrame) -> dict:\n    \"\"\"model evaluating with statistical methods\"\"\"\n    try:\n        logger.info('Model Evaluation Started')\n        y_pred = clf.predict(X_test)\n        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred)\n        recall = recall_score(y_test, y_pred)\n        roc_auc = roc_auc_score(y_test, y_pred_proba)",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "save_metrics",
        "kind": 2,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "def save_metrics(metrics:dict,file_path:str):\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        with open(file_path, 'w') as file:\n            json.dump(metrics, file, indent=4)\n        logging.info(\"model evaluation metrics saved to %s\",file_path)\n    except Exception as e:\n        logging.error(\"error occurs in saving the model evaluation metrics %s\",e)\n        raise\ndef main():",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "def main():\n    try:\n        clf = load_model('./models/model.pkl')\n        test_data = load_data('./data/processed/test_tfidf.csv')\n        x_test = test_data.iloc[:, :-1].values\n        y_test = test_data.iloc[:, -1].values\n        metrics = evaluate_model(clf, x_test, y_test)\n        save_metrics(metrics, 'reports/metrics.json')\n    except Exception as e:\n        logging.error(\"error occurs in model evaluation %s\",e)",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "log_dir = 'logs'\nos.makedirs(log_dir,exist_ok=True)\nlogger=logging.getLogger('model_evaluation')\nlogger.setLevel('DEBUG')\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"model_evaluation.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "console_handler",
        "kind": 5,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "console_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"model_evaluation.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "file_handler",
        "kind": 5,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "file_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Model Evaluation Started')\ndef load_model(model_path: str):\n    try:",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "src.model_evaluation",
        "description": "src.model_evaluation",
        "peekOfCode": "formatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Model Evaluation Started')\ndef load_model(model_path: str):\n    try:\n        with open(model_path, \"rb\") as model_file:\n            model = joblib.load(model_file)",
        "detail": "src.model_evaluation",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "def load_data(file_path: str) -> pd.DataFrame:\n    try:\n        df=pd.read_csv(file_path)\n        logging.info(\"data loaded from %s file path of size %s\",file_path,df.shape)\n        return df\n    except pd.errors.ParserError as e:\n        logging.error(\"faied to parse the file%s\",e)\n        raise\n    except Exception as e:\n        logging.error(f\"Error loading data from {file_path}: {e}\")",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "def train_model(X_train: np.ndarray, y_train: np.ndarray, params:dict) -> RandomForestClassifier:\n    try:\n        if X_train.shape[0] != y_train.shape[0]:\n            raise ValueError(\"X_train and y_train must have the same number of samples\")\n        logging.info(\"initialing the random forest classifier with parameters %s\",params)\n        clf = RandomForestClassifier(n_estimators=params['n_estimators'], random_state=params['random_state'])\n        clf.fit(X_train,y_train)\n        logging.info(\"model trained successfully\")\n        return clf\n    except Exception as e:",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "save_model",
        "kind": 2,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "def save_model(model,file_path:str):\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        with open(file_path, 'wb') as file:\n            pickle.dump(model, file)\n        logging.info(\"model saved to %s\",file_path)\n    except Exception as e:\n        logging.error(\"error occurs in saving the model %s\",e)\n        raise\ndef main():",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "def main():\n    try:\n        params = {'n_estimators':25, 'random_state':2}\n        trained_data = load_data('./data/processed/train_tfidf.csv')\n        X_train = trained_data.iloc[:, :-1].values\n        y_train = trained_data.iloc[:, -1].values\n        clf = train_model(X_train, y_train, params)\n        save_model(clf,'models/model.pkl')\n        logging.info(\"model saved successfully\")\n    except Exception as e:",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "log_dir = 'logs'\nos.makedirs(log_dir,exist_ok=True)\nlogger=logging.getLogger('model_training')\nlogger.setLevel('DEBUG')\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"model_training.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "console_handler",
        "kind": 5,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "console_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"model_training.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "file_handler",
        "kind": 5,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "file_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogging.info(\"starting the model training\")\ndef load_data(file_path: str) -> pd.DataFrame:\n    try:",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "src.model_training",
        "description": "src.model_training",
        "peekOfCode": "formatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogging.info(\"starting the model training\")\ndef load_data(file_path: str) -> pd.DataFrame:\n    try:\n        df=pd.read_csv(file_path)\n        logging.info(\"data loaded from %s file path of size %s\",file_path,df.shape)",
        "detail": "src.model_training",
        "documentation": {}
    },
    {
        "label": "transform_text",
        "kind": 2,
        "importPath": "src.preprocessing",
        "description": "src.preprocessing",
        "peekOfCode": "def transform_text(text):\n    \"\"\"transforming the text \"\"\"\n    ps = PorterStemmer()\n    text = text.lower()\n    text = nltk.word_tokenize(text)\n    text = [word for word in text if word.isalnum()]\n    text = [word for word in text if word not in stopwords.words('english') and word not in string.punctuation]\n    text = [ps.stem(word) for word in text]\n    return \" \".join(text)\ndef pre_processing(df:pd.DataFrame, text_col='text', target_col='target'):",
        "detail": "src.preprocessing",
        "documentation": {}
    },
    {
        "label": "pre_processing",
        "kind": 2,
        "importPath": "src.preprocessing",
        "description": "src.preprocessing",
        "peekOfCode": "def pre_processing(df:pd.DataFrame, text_col='text', target_col='target'):\n    \"\"\"preprocessing the data by encoding the target column and transforming the text column\"\"\"\n    try:\n        logging.info(\"Preprocessing started\")\n        encoder = LabelEncoder()\n        df[target_col] = encoder.fit_transform(df[target_col])\n        logging.debug(\"Target column encoded\")\n        df = df.drop_duplicates(keep='first')\n        logging.debug(\"Duplicates removed\")\n        df.loc[:, text_col] = df[text_col].apply(transform_text)",
        "detail": "src.preprocessing",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "src.preprocessing",
        "description": "src.preprocessing",
        "peekOfCode": "def main():\n    try:\n        \"\"\"main function to load data, transform it\"\"\"\n        train_data = pd.read_csv('./data/raw/train.csv')\n        test_data = pd.read_csv('./data/raw/test.csv')\n        logging.debug(\"Data loaded from csv file\")\n        train_processed_data = pre_processing(train_data)\n        test_processed_data = pre_processing(test_data)\n        data_path = os.path.join('./data/processed','interim')\n        os.makedirs(data_path, exist_ok=True)",
        "detail": "src.preprocessing",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "src.preprocessing",
        "description": "src.preprocessing",
        "peekOfCode": "log_dir = 'logs'\nos.makedirs(log_dir,exist_ok=True)\nlogger=logging.getLogger('preprocessing')\nlogger.setLevel('DEBUG')\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"preprocessing.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')",
        "detail": "src.preprocessing",
        "documentation": {}
    },
    {
        "label": "console_handler",
        "kind": 5,
        "importPath": "src.preprocessing",
        "description": "src.preprocessing",
        "peekOfCode": "console_handler = logging.StreamHandler()\nconsole_handler.setLevel('DEBUG')\nlog_file_path=os.path.join(log_dir, \"preprocessing.log\")\nfile_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)",
        "detail": "src.preprocessing",
        "documentation": {}
    },
    {
        "label": "file_handler",
        "kind": 5,
        "importPath": "src.preprocessing",
        "description": "src.preprocessing",
        "peekOfCode": "file_handler = logging.FileHandler(log_file_path)\nfile_handler.setLevel('DEBUG')\nformatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Preprocessing Started')\ndef transform_text(text):\n    \"\"\"transforming the text \"\"\"",
        "detail": "src.preprocessing",
        "documentation": {}
    },
    {
        "label": "formatter",
        "kind": 5,
        "importPath": "src.preprocessing",
        "description": "src.preprocessing",
        "peekOfCode": "formatter = logging.Formatter('%(asctime)s -%(levelname)s- %(message)s')\nconsole_handler.setFormatter(formatter)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\nlogger.info('Preprocessing Started')\ndef transform_text(text):\n    \"\"\"transforming the text \"\"\"\n    ps = PorterStemmer()\n    text = text.lower()",
        "detail": "src.preprocessing",
        "documentation": {}
    }
]